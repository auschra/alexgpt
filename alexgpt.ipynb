{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy \n",
    "import transformers\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"todos\\n    EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention`\\n     into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\\n     \\n    EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion \\n\\n    if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse\\n     order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify \\n     the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss \\n     at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does \\n     your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\\n\\n    EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer \\n    on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning\\n     rate. Can you obtain a lower validation loss by the use of pretraining?\\n     \\n    EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"todos\n",
    "    EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention`\n",
    "     into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "     \n",
    "    EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion \n",
    "\n",
    "    if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse\n",
    "     order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify \n",
    "     the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss \n",
    "     at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does \n",
    "     your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "\n",
    "    EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer \n",
    "    on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning\n",
    "     rate. Can you obtain a lower validation loss by the use of pretraining?\n",
    "     \n",
    "    EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# B, T, C\n",
    "batch_size = 1\n",
    "block_size = 32\n",
    "n_embd = 64\n",
    "head_size = 8\n",
    "learning_rate =1e-4\n",
    "n_layer = 2\n",
    "n_head = 8\n",
    "dropout = 0.2\n",
    "max_iters = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (488544 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input token length 488544\n",
      "vocab size = 14368\n",
      "train data size: torch.Size([390835])\n",
      "val data size: torch.Size([97709])\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Tokenize inputs \n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenized_data = tokenizer.encode(data)\n",
    "print(f\"input token length {len(tokenized_data)}\")\n",
    "\n",
    "# Create vocabulary and mappings\n",
    "vocab = sorted(set(tokenized_data))\n",
    "vocab_size = len(vocab)\n",
    "print(f\"vocab size = {vocab_size}\")\n",
    "\n",
    "# These mappings are necessary\n",
    "map_data = {old_id: new_id for new_id, old_id in enumerate(vocab)}\n",
    "unmap_data = {new_id: old_id for new_id, old_id in enumerate(vocab)}\n",
    "\n",
    "# Map tokens to new vocabulary\n",
    "mapped_data = [map_data[token] for token in tokenized_data]\n",
    "\n",
    "# Create train test splits\n",
    "n = int(0.8 * len(mapped_data))\n",
    "train_data = mapped_data[:n]\n",
    "val_data = mapped_data[n:]\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.long)\n",
    "val_data = torch.tensor(val_data, dtype=torch.long)\n",
    "print(f\"train data size: {train_data.shape}\")\n",
    "print(f\"val data size: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n",
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))     # get batch_size starting points\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(xb.shape)\n",
    "print(yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1D:\n",
    "    # normalise each batch -> that way to get diff from mean and unit variance\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1): # params\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)    \n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim= True)    # batch mean\n",
    "        xvar = x.var(1, keepdim=True)          # batch var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalise a row, columwise \n",
    "        self.out = self.gamma * xhat + self. beta\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):       \n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "module = LayerNorm1D(100)   # 100 dimensions\n",
    "x = torch.randn(32, 100)    # 32 batch, 100 dims\n",
    "\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # linear layers\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)      # (B, T, head_size)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)   # (B, T, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)   # (B, T, head_size)\n",
    "        self.register_buffer('tril', torch.ones(block_size, block_size))    # (B, T, T)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # B, T, head_size   \n",
    "        q = self.key(x) # B, T, head_size \n",
    "        tril = self.tril[:T, :T].to(device)    \n",
    "\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) #keepdim=True) # (B, T, head_size) @ (B, head_size, T) -> keep batch dim so (B, T, T)     \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))     # lower triangular matrix (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1)              # normalise \n",
    "        wei = self.dropout(wei)                      # dropout\n",
    "\n",
    "        v = self.key(x) # B, T, head_size \n",
    "\n",
    "        out = wei @ v            # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(n_head))\n",
    "        self.proj = nn.Linear(n_head * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)     # (B, T, C) # each head_size gets added together\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into causal self attention\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init___(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList()  # B, T, head_size, num_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd), \n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(4*n_embd, n_embd),\n",
    "                                nn.Dropout(dropout),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)          # B, T, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # transformer block, attention -> mlp\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)    # create this many heads, fits into n_ebd\n",
    "        self.ffd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffd(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        assert vocab_size is not None\n",
    "        assert block_size is not None\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)       # final layer norm after mlp?\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        pos = torch. arange(0, T, dtype=torch.long, device=device)  # (T)\n",
    "\n",
    "        # forward GPT model\n",
    "        tok_emb = self.token_embedding_table(idx)    # B, T, n_embd\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))   # (T, n_embd)\n",
    "        x = tok_emb + pos_emb # B, T, C \n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is B, T indices of context, unembedded\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get block size of last input\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get predicitons\n",
    "            logits, _ = self(idx_cond)   \n",
    "            \n",
    "            # last time step?\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from dist\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # B, 1\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, loss: 9.842464447021484\n",
      "Iter 1000, loss: 7.147793292999268\n",
      "Iter 2000, loss: 6.804561138153076\n",
      "Iter 3000, loss: 6.6118597984313965\n",
      "Iter 4000, loss: 7.111612319946289\n",
      "Iter 5000, loss: 6.9859771728515625\n",
      "Iter 6000, loss: 5.455924034118652\n",
      "Iter 7000, loss: 5.6383771896362305\n",
      "Iter 8000, loss: 6.018691062927246\n",
      "Iter 9000, loss: 6.063382148742676\n",
      "! You?” ask thatov\n",
      "s, I believe\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens):\n",
    "    # First unmap from our custom vocabulary back to original token IDs\n",
    "    original_tokens = [unmap_data[token] for token in tokens]\n",
    "    # Then decode using the tokenizer\n",
    "    return tokenizer.decode(original_tokens)\n",
    "\n",
    "max_iters= 10000\n",
    "model = GPT().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evaluate the loss on train / val every so often\n",
    "    xb, yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % 1000 == 0:\n",
    "        print(f'Iter {iter}, loss: {loss.item()}')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # start with zeros tensor\n",
    "\n",
    "# ignore gradients for efficiency\n",
    "with torch.no_grad():\n",
    "    \n",
    "    y = model.generate(context, max_new_tokens=10)\n",
    "    print(decode(y[0].tolist()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
