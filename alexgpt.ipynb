{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/auschra/ml/llm/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoTokenizer\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Todos\n",
    "- RMSnorm\n",
    "- wandb logging\n",
    "- rope pos encoding\n",
    "- ring attention\n",
    "- kv cache\n",
    "- weight sharing\n",
    "- config class\n",
    "- \n",
    "- '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# B, T, C\n",
    "batch_size = 32\n",
    "block_size = 512\n",
    "\n",
    "n_embd = 768\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "\n",
    "learning_rate =1e-4\n",
    "\n",
    "dropout = 0.2\n",
    "max_iters = 10000\n",
    "compile = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301769 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input token length 301769\n",
      "vocab size = 12132\n",
      "train data size: torch.Size([241415])\n",
      "val data size: torch.Size([60354])\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# tokenize\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenized_data = tokenizer.encode(data)\n",
    "print(f\"input token length {len(tokenized_data)}\")\n",
    "\n",
    "# create vocab\n",
    "vocab = sorted(set(tokenized_data))\n",
    "vocab_size = len(vocab)\n",
    "print(f\"vocab size = {vocab_size}\")\n",
    "\n",
    "# mapping large vocab size tokens to input vocab size\n",
    "map_data = {old_id: new_id for new_id, old_id in enumerate(vocab)}\n",
    "unmap_data = {new_id: old_id for new_id, old_id in enumerate(vocab)}\n",
    "\n",
    "# map inputs\n",
    "mapped_data = [map_data[token] for token in tokenized_data]\n",
    "\n",
    "# train test splits\n",
    "n = int(0.8 * len(mapped_data))\n",
    "train_data = mapped_data[:n]\n",
    "val_data = mapped_data[n:]\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.long)\n",
    "val_data = torch.tensor(val_data, dtype=torch.long)\n",
    "print(f\"train data size: {train_data.shape}\")\n",
    "print(f\"val data size: {val_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear\n",
      "[12131, 2529, 9033, 261, 3967, 309, 4004, 515, 2287, 2, 2974]\n",
      "<|begin_of_text|>\n",
      "First\n",
      " Citizen\n",
      ":\n",
      "\n",
      "Before\n",
      " we\n",
      " proceed\n",
      " any\n",
      " further\n",
      ",\n",
      " hear\n",
      "<|begin_of_text|>First Citizen:\n",
      "Before we proceed any further, hear\n",
      "<|begin_of_text|>\n",
      "dis\n",
      "ent\n",
      "ang\n",
      "lement\n"
     ]
    }
   ],
   "source": [
    "print(data[:50])\n",
    "tok50 = ([map_data[token] for token in tokenizer.encode(data[:50])])\n",
    "print(tok50)\n",
    "\n",
    "for c in tok50:\n",
    "    print(tokenizer.decode([unmap_data[c]]))\n",
    "print(tokenizer.decode([unmap_data[token] for token in tok50]))\n",
    "\n",
    "\n",
    "word = 'disentanglement'\n",
    "\n",
    "toks = tokenizer.encode(word)\n",
    "for c in toks:\n",
    "    print(tokenizer.decode(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))     # get batch_size starting points\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(xb.shape)\n",
    "print(yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked multi head attention\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head):\n",
    "        super().__init__()\n",
    "        # heads fit into n_embd\n",
    "        assert n_embd % n_head == 0\n",
    "        # create k, q, v projections for a batch\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        # output\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # nanogpt_scal_init\n",
    "        # regularise, n_head, and n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape       # batch, sequence length, embedding size\n",
    "        # calc, k, q, v for heads in a batch\n",
    "        qkv = self.c_attn(x)        # (B, T, C) -> (C, 3*C) -> (B, T ,3*C)\n",
    "        q, k, v = qkv.split(n_embd, dim=2)   # (B, T, C)\n",
    "\n",
    "        # reorder dims to be (B, C, T, n_head)\n",
    "        q = q.view(B, T, self.n_head, C // n_head).transpose(1, 2)     # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // n_head).transpose(1, 2)     # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // n_head).transpose(1, 2)     # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)    # mask future tokens\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)                # (B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd),   # hidden\n",
    "                                nn.GELU(),                      # nonlinearity\n",
    "                                nn.Linear(4*n_embd, n_embd),    # out\n",
    "                                nn.Dropout(dropout),)           # regularise\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x            # (B, T, C)\n",
    "\n",
    "\n",
    "\n",
    "class Layernorm(nn.Module):\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "    # so give batch of (B, T, C)\n",
    "    # layernorm normalises (T, C)\n",
    "    # unit mean and variance\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "\n",
    "    # like layernorm but uses root mean square\n",
    "\n",
    "# take in a batch and return the batch normalise along the embedding dimension\n",
    "    def forward(self):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # normalise the layers along the embedding layer \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(n_embd)        # normalise embedding dim\n",
    "        self.csa = CausalSelfAttention(n_head)\n",
    "        self.ln_2 = nn.LayerNorm(n_embd)\n",
    "        self.ffd = MLP()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.csa(self.ln_1(x))\n",
    "        x = x + self.ffd(self.ln_2(x))\n",
    "\n",
    "        return x \n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        assert vocab_size is not None\n",
    "        assert block_size is not None\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)       # final layer norm after mlp?\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # weight sharing \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        pos = torch. arange(0, T, dtype=torch.long, device=device)  # (T)\n",
    "\n",
    "        # forward GPT model\n",
    "        tok_emb = self.token_embedding_table(idx)    # B, T, n_embd\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))   # (T, n_embd)\n",
    "        x = tok_emb + pos_emb # B, T, C \n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is B, T indices of context, unembedded\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get block size of last input\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get predicitons\n",
    "            logits, _ = self(idx_cond)   \n",
    "            \n",
    "            # last time step?\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from dist\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # B, 1\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, loss: 9.577692985534668\n",
      "!Measure rob happenedFine felt aloneodiac acheswordrops\n",
      "Iter 100, loss: 5.760305404663086\n",
      "! if above;\n",
      "How sh me and beauty ofRY\n",
      "Iter 200, loss: 5.056351661682129\n",
      "! soINGS things it be looks, good, for\n",
      "Iter 300, loss: 4.927411079406738\n",
      "! so truth: then be Norfolk you\n",
      "counter hundred\n",
      "Iter 400, loss: 4.498869895935059\n",
      "! another\n",
      "corner patience, and stand creating me,\n",
      "Iter 500, loss: 4.344297885894775\n",
      "! pu child with pure mastered at pity is the\n",
      "Iter 600, loss: 4.250061988830566\n",
      "! my guilt slaves, my pains, I dream!\n",
      "\n",
      "Iter 700, loss: 3.9501209259033203\n",
      "! think orices, if, know of you were\n",
      "Iter 800, loss: 3.828605890274048\n",
      "! in peace of 'good harloter in him\n",
      "Iter 900, loss: 3.836083173751831\n",
      "! I wisely unto thee, I hope\n",
      "The v\n",
      "Iter 1000, loss: 3.6055996417999268\n",
      "! I am no most estate. double fitness\n",
      "The\n",
      "Iter 1100, loss: 3.5751936435699463\n",
      "! death to blame up,\n",
      "Becomes it me\n",
      "Iter 1200, loss: 3.4682202339172363\n",
      "! I will do beat me know these not!\n",
      "\n",
      "B\n",
      "Iter 1300, loss: 3.2915918827056885\n",
      "! my back than to give bold to age too unt\n",
      "Iter 1400, loss: 3.234036922454834\n",
      "! theious through each life,\n",
      "HisTouches to rate\n",
      "Iter 1500, loss: 2.9992427825927734\n",
      "! PrMerry?\n",
      "\n",
      "MAMILLIUS:\n",
      "\n",
      "Iter 1600, loss: 2.753345012664795\n",
      "! when Ioms, that telling dearest I thank\n",
      "Iter 1700, loss: 2.579303503036499\n",
      "! I give earth my guiltless\n",
      "To this good\n",
      "Iter 1800, loss: 2.471144676208496\n",
      "! my soul,\n",
      "Do not thy royal presence first that\n",
      "Iter 1900, loss: 2.240131139755249\n",
      "! what is grown\n",
      "Desire to't.\n",
      "And\n",
      "Iter 2000, loss: 2.042125701904297\n",
      "! that man is yest thou dost; he is\n",
      "Iter 2100, loss: 1.8483893871307373\n",
      "! a furlongs ere my sweet-heartant!\n",
      "\n",
      "Iter 2200, loss: 1.648777961730957\n",
      "! no shelter to exclaim I do not use that\n",
      "Iter 2300, loss: 1.356835961341858\n",
      "! he is already: he\n",
      "int bosom in\n",
      "Iter 2400, loss: 1.3066307306289673\n",
      "! and myself!\n",
      "The unity in a three youth\n",
      "\n",
      "Iter 2500, loss: 1.1572442054748535\n",
      "!, who comes our loving friends,\n",
      "Might thus\n",
      "Iter 2600, loss: 0.9890209436416626\n",
      "! fast my heart's first,\n",
      "And beggar to\n",
      "Iter 2700, loss: 0.8879804611206055\n",
      "! Sir our general we will be to do;\n",
      "And\n",
      "Iter 2800, loss: 0.7752779722213745\n",
      "! Iventh be i' the field to the helm\n",
      "Iter 2900, loss: 0.6413335800170898\n",
      "! call'd,\n",
      "The senators shall have you accuse you\n",
      "Iter 3000, loss: 0.5888112783432007\n",
      "! my blest thou not as I do,\n",
      "Nor\n",
      "Iter 3100, loss: 0.533699631690979\n",
      "! come, what the poor's point of all,\n",
      "\n",
      "Iter 3200, loss: 0.4916168749332428\n",
      "! London's crown?\n",
      "\n",
      " aloud.\n",
      "\n",
      "HENRY PERCY\n",
      "Iter 3300, loss: 0.42465221881866455\n",
      "! we hear him hence to our voices,\n",
      "What c\n",
      "Iter 3400, loss: 0.40209004282951355\n",
      "! forgot!\n",
      "My wife, my lord: God's\n",
      "Iter 3500, loss: 0.34442371129989624\n",
      "! what news?\n",
      "\n",
      "Messenger:\n",
      "Before I have I saw\n",
      "Iter 3600, loss: 0.3448016047477722\n",
      "! make weep; battles bow up,\n",
      "Hold,\n",
      "Iter 3700, loss: 0.30740222334861755\n",
      "! theanged, we may Viraw you shall\n",
      "\n",
      "Iter 3800, loss: 0.2603273093700409\n",
      "! shapes farewell, you so: but, sir.\n",
      "\n",
      "\n",
      "Iter 3900, loss: 0.2701505124568939\n",
      "! session, and lay down my\n",
      "the utmost of\n",
      "Iter 4000, loss: 0.23402540385723114\n",
      "! lets thee not for a little world\n",
      "pretty bar\n",
      "Iter 4100, loss: 0.23515059053897858\n",
      "! Was's no venom.\n",
      "I am past hope to\n",
      "Iter 4200, loss: 0.22640590369701385\n",
      "! misfortune I am told me better,\n",
      "I would\n",
      "Iter 4300, loss: 0.21222957968711853\n",
      "! backs! who lives,\n",
      "The County Paris slain by\n",
      "Iter 4400, loss: 0.21226921677589417\n",
      "! why commands my lord mayor along with too?\n",
      "\n",
      "K\n",
      "Iter 4500, loss: 0.19513562321662903\n",
      "! my soul,\n",
      "How as I did in your decre\n",
      "Iter 4600, loss: 0.18188907206058502\n",
      "!Away! My bark, then so young boy!\n",
      "\n",
      "Iter 4700, loss: 0.1760493665933609\n",
      "! and, steep as to all;\n",
      "Run to within\n",
      "Iter 4800, loss: 0.17203345894813538\n",
      "! he is come!\n",
      "but I madam, he\n",
      "Iter 4900, loss: 0.15971200168132782\n",
      "! fear to discredits.\n",
      "\n",
      "KING RICHARD\n",
      "Iter 5000, loss: 0.1598082035779953\n",
      "! for thou provokest not seem'st,\n",
      "Nor\n",
      "Iter 5100, loss: 0.14549557864665985\n",
      "! who throngs spirit!\n",
      "\n",
      "MARCIUS:\n",
      "Most\n",
      "Iter 5200, loss: 0.1579711139202118\n",
      "! who sake with you?\n",
      "\n",
      "CLARENCE:\n",
      "For\n",
      "Iter 5300, loss: 0.14917126297950745\n",
      "! O heavens!\n",
      "\n",
      "DORSET:\n",
      " angel, Warwick\n",
      "Iter 5400, loss: 0.13894739747047424\n",
      "! you take me for my knee\n",
      "You did hear\n",
      "Iter 5500, loss: 0.12867355346679688\n",
      "! Why I not believe me do\n",
      "In weightst\n",
      "Iter 5600, loss: 0.13699224591255188\n",
      "! diest thou there'st pale.\n",
      "\n",
      "KING\n",
      "Iter 5700, loss: 0.12675440311431885\n",
      "! the scarcely bear\n",
      "to the truth. The prince\n",
      "Iter 5800, loss: 0.1209467425942421\n",
      "! Benvolio; our good sweet as it is\n",
      "Iter 5900, loss: 0.12417598068714142\n",
      "! lack, here it not your company:\n",
      "Away!\n",
      "\n",
      "\n",
      "Iter 6000, loss: 0.11185568571090698\n",
      "! here with us enemies,\n",
      "Still live your standards,\n",
      "Iter 6100, loss: 0.10822071880102158\n",
      "! urge,\n",
      "Nor we more infected with us.\n",
      "\n",
      "JO\n",
      "Iter 6200, loss: 0.11052257567644119\n",
      "! plight!\n",
      "\n",
      "EDWARD:\n",
      "Welcome, gentle Warwick!\n",
      "Iter 6300, loss: 0.09865768253803253\n",
      "! the raying, craves\n",
      "To be ble\n",
      "Iter 6400, loss: 0.10706934332847595\n",
      "! I drink after this do\n",
      "Do give my metal\n",
      "Iter 6500, loss: 0.09723281115293503\n",
      "! forgot!\n",
      "Who thrives not to heaven, for\n",
      "Iter 6600, loss: 0.1021144837141037\n",
      "! Fear shall be six or death!\n",
      "Condemned\n",
      "Iter 6700, loss: 0.0954236313700676\n",
      "! and discovery too hot, bigger, think peer,\n",
      "Iter 6800, loss: 0.09633180499076843\n",
      "! and ha! not so?\n",
      "Why, no,\n",
      "Iter 6900, loss: 0.08778979629278183\n",
      "! lover!\n",
      "What noise of loving child is this noble\n",
      "Iter 7000, loss: 0.08658167719841003\n",
      "! ay, and these petty brands\n",
      "Have the bark\n",
      "Iter 7100, loss: 0.08937233686447144\n",
      "! proclamation, ready is even here?\n",
      "\n",
      "Nurse:\n",
      "\n",
      "Iter 7200, loss: 0.08712764084339142\n",
      "! O innocent employ a letter our subject,\n",
      "A just\n",
      "Iter 7300, loss: 0.08122730255126953\n",
      "! away! traitors! false and! lover!\n",
      "\n",
      "Iter 7400, loss: 0.08599580824375153\n",
      "! Gloucester, that's tom\n",
      "Hath made\n",
      "Iter 7500, loss: 0.08645862340927124\n",
      "! haRomeo is coming to;\n",
      "And bid\n",
      "Iter 7600, loss: 0.08409509807825089\n",
      "! a crutch! why,\n",
      "a bootless;\n",
      "Iter 7700, loss: 0.07455694675445557\n",
      "! Fear camest thou cold conge.\n",
      "\n",
      "GREG\n",
      "Iter 7800, loss: 0.06826954334974289\n",
      "! O hateful day!\n",
      "Death is the haw'd\n",
      "Iter 7900, loss: 0.07343775033950806\n",
      "! fire, ha! my lord is broke to all\n",
      "Iter 8000, loss: 0.07029189169406891\n",
      "! here\n",
      "Here comes Bohemia to o' the\n",
      "Iter 8100, loss: 0.0710439383983612\n",
      "! pound us the Volsces\n",
      "No public benefit\n",
      "Iter 8200, loss: 0.06969594210386276\n",
      "! I sp!\n",
      "O pity, pity, pity!\n",
      "\n",
      "\n",
      "Iter 8300, loss: 0.06927260756492615\n",
      "! whose foul mouth, my memory!\n",
      "\n",
      "AUFIDI\n",
      "Iter 8400, loss: 0.06774132698774338\n",
      "! double spring of childish bowels your highness cur\n",
      "Iter 8500, loss: 0.06567777693271637\n",
      "! men yield them going:\n",
      "Edward and Richard, bear\n",
      "Iter 8600, loss: 0.07055283337831497\n",
      "! what thought of him, gone?\n",
      "On Thursday next\n",
      "Iter 8700, loss: 0.05877655744552612\n",
      "! They are come to drink\n",
      "Do more than done\n",
      "Iter 8800, loss: 0.06314550340175629\n",
      "! fearfully, Wednesday, Wednesday, kiss me too\n",
      "Iter 8900, loss: 0.05630439147353172\n",
      "! blow it is to heaven\n",
      "By sovereignty of Mar\n",
      "Iter 9000, loss: 0.0641658753156662\n",
      "! Can it seems.\n",
      "Tis since the torch-b\n",
      "Iter 9100, loss: 0.05784688517451286\n",
      "! grace\n",
      " many thousand fitter for him.\n",
      "\n",
      "First\n",
      "Iter 9200, loss: 0.06058301776647568\n",
      "! fie! all the trades in Rome,\n",
      "The second\n",
      "Iter 9300, loss: 0.05739237368106842\n",
      "! Plantagenet!\n",
      "Away, pity, gentle heaven\n",
      "Iter 9400, loss: 0.053789250552654266\n",
      "! Juliet, O, ah, no,\n",
      "What comfortable\n",
      "Iter 9500, loss: 0.05675430968403816\n",
      "! thy childishness grant that they were\n",
      "Is like\n",
      "Iter 9600, loss: 0.05089448764920235\n",
      "! I say's there?\n",
      "\n",
      "Servant:\n",
      "One Isabel\n",
      "Iter 9700, loss: 0.058809418231248856\n",
      "! victory I have done with thee? when\n",
      "To\n",
      "Iter 9800, loss: 0.0582759864628315\n",
      "! the gods assu!--\n",
      "\n",
      "First Citizen:\n",
      "Ye\n",
      "Iter 9900, loss: 0.0550319142639637\n",
      "! alack! my kingdom for a dead man!\n",
      "! not our first died be gone with our blood\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens):\n",
    "    # First unmap from our custom vocabulary back to original token IDs\n",
    "    original_tokens = [unmap_data[token] for token in tokens]\n",
    "    # Then decode using the tokenizer\n",
    "    return tokenizer.decode(original_tokens)\n",
    "\n",
    "max_iters= 10000\n",
    "model = GPT().to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# B , T, head_size, num_heads\n",
    "for iter in range(max_iters):\n",
    "    model.train()\n",
    "    xb, yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iter {iter}, loss: {loss.item()}')\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            context = torch.zeros((1, 1), dtype=torch.long, device=device)  # start with zeros tensor\n",
    "        \n",
    "            y = model.generate(context, max_new_tokens=10)\n",
    "            print(decode(y[0].tolist()))\n",
    "\n",
    "model.eval()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # start with zeros tensor\n",
    "\n",
    "# ignore gradients for efficiency\n",
    "with torch.no_grad():\n",
    "    \n",
    "    y = model.generate(context, max_new_tokens=100)\n",
    "    print(decode(y[0].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! and accursed effect\n",
      "Cannot be accused, the great place of your dwellingure,\n",
      "That you shall welcome! So with the Capulets\n",
      "If not have your scope Buckingham behests,\n",
      "For this day you: Cleomenes and Dion, beat them to Westminster,\n",
      "For the king, great Hostiliumny doth haste.\n",
      "\n",
      "First Soldier:\n",
      "To the queen to send the contrary me\n",
      "Should not yet distinctly ranges. When he was not most noble\n",
      "Only in a town: as any thing possible\n",
      "The which is dishonour, so grieving\n",
      "With a them; and falling fabric. 'Tis time:\n",
      "I have of but yet I be endured\n",
      "I was a son o' the Volsce like of many of mine,\n",
      "The effects of them too gross and so dishonour\n",
      "We have yielded to fear'd their eyes and request\n",
      "With painted to break the cushion, not\n",
      "Of, which I would have given already.\n",
      "\n",
      "POLIXENES:\n",
      "On, my lord!\n",
      "\n",
      "HERMIONE:\n",
      "Was not! there?\n",
      "\n",
      "POLIXENES:\n",
      "The king hath been born to-morrow affords.\n",
      "\n",
      "HERMIONE:\n",
      "Nay, but that,\n",
      "There is even your graces\n",
      "You were boys:\n",
      "Nay, but you will?\n",
      "\n",
      "POLIXENES:\n",
      "We were as twinn'd with limber vows,\n",
      "And bleat the prisoner.\n",
      "\n",
      "HERMIONE:\n",
      "O no one good shepherd!\n",
      "What cheer? how bleeds; then?\n",
      "What stay we are her? mark you say is't be England,\n",
      "If you would seek to be too cold.\n",
      "\n",
      "HERMIONE:\n",
      "You smell as you would seek to seek the\n",
      "stars with Bohemia.\n",
      "\n",
      "POLIXENES:\n",
      "Nay, then,\n",
      "You speak not, fair one cannot say you;\n",
      "Your precious selfsame, if you\n",
      "With flowers of the watery, should\n",
      "Are you be punish.\n",
      "\n",
      "HERMIONE:\n",
      "But your gaoler, then,\n",
      "But your kind hostess. When\n",
      "Of my lord's tricks and yours when you were boys:\n",
      "You were pretty lord,\n",
      "Two lads that he were not rule to grieve'd to-day,\n",
      "You were wilful-negligent,\n",
      "By circumstance and devour\n",
      "And those that touch'd\n",
      "The bitter bread of the table of supreme'd\n",
      "Which industryful conference with great king\n",
      "Do wound vain weak spirits;\n",
      "Her and offend thence have engross'd\n",
      "Of great Apollo's priest andWell,\n",
      "You have power to meet your lord.\n",
      "\n",
      "LEONTES:\n",
      "No longer stay.\n",
      "\n",
      "Officer:\n",
      "\n",
      "HERMIONE:\n",
      "Since what I am to say must be but that?\n",
      "\n",
      "LEONTES:\n",
      "Never, my lord.\n",
      "\n",
      "HERMIONE:\n",
      "What was on, I twice said well? Beseech your highness,\n",
      "My women may be full of choice,\n",
      "Which I'll lay down.\n",
      "\n",
      "LEONTES:\n",
      "Your actions are my dearest, my dreams;\n",
      "You had a bastard by Polixenes,\n",
      "And I but dream'd it. As you were past all shame\n",
      "To me to perform'd the world.\n",
      "\n",
      "HERMIONE:\n",
      "My life stands in mine own sex\n",
      "Of my accusation and\n",
      "Of my accusation and do't.\n",
      "\n",
      "LEONTES:\n",
      "Oursuades me, I think sooth, I think,\n",
      "Such comfort me the truth; and so 'tells\n",
      "To what I married go sapling one too tender\n",
      "The blessed gods, which I would do't.\n",
      "\n",
      "POLIXENES:\n",
      "Then make't and since\n",
      "As he had not the power.ignor heard it good.\n",
      "\n",
      "HERMIONE:\n",
      "'Tis time enough;\n",
      "Which comes to me not that I'll stay the soil'd\n",
      "Of my study and leave, I since\n",
      "Myself my power and my lord, most love you\n",
      "You did mistake.\n",
      "\n",
      "LEONTES:\n",
      "No, in being so oft the time worse than you\n",
      "Yourself condemned lord, and I'll think so most love!\n",
      "Your followers you not have said\n",
      "But since an honest, and so quake: I do, if you deny,\n",
      "I dare not stand by your wives, you\n",
      "You had been in being spotted\n",
      "Ere they can dream of love or make known\n",
      "You kill him as he think, and a\n",
      "To say and to have shed many mine honour,\n",
      "The wars and whose spiritual counsel had not guilty\n",
      "The scene you, though on his queen's majesty,\n",
      "But to a hopeful gentleman, and so drew on me\n",
      "And with a just computation of his ill of his chivalry;\n",
      "And seem a graver with his blood\n",
      "Is piled upon his appetite in rage,\n",
      "Fell'd, unta with his sleep,\n",
      "Who nothing hurt with his lineal;\n",
      "And that, unvalued himself little urged, have prevented,\n",
      "To fright me with his utmost a woe,\n",
      "That had a hovering temporizer, love! and so I my life,\n",
      "To show myself mine own life run mad:\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    y = model.generate(context, max_new_tokens=1000)\n",
    "    print(decode(y[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
