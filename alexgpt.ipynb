{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy \n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27c7ff32690>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"todos\\n    EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention`\\n     into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\\n     \\n    EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion \\n\\n    if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse\\n     order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify \\n     the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss \\n     at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does \\n     your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\\n\\n    EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer \\n    on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning\\n     rate. Can you obtain a lower validation loss by the use of pretraining?\\n     \\n    EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?\\n\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"todos\n",
    "    EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention`\n",
    "     into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "     \n",
    "    EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion \n",
    "\n",
    "    if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse\n",
    "     order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify \n",
    "     the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss \n",
    "     at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does \n",
    "     your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "\n",
    "    EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer \n",
    "    on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning\n",
    "     rate. Can you obtain a lower validation loss by the use of pretraining?\n",
    "     \n",
    "    EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# B, T, C\n",
    "batch_size = 4\n",
    "block_size = 100\n",
    "n_embd = 16\n",
    "head_size = 4\n",
    "learning_rate =1e-4\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "dropout = 0.0\n",
    "max_iters = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# encoding \n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "data = enc.encode(data)\n",
    "\n",
    "toks = sorted(list(set(data)))\n",
    "vocab_size = len(toks)\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "train_data = torch.tensor(train_data)\n",
    "val_data = torch.tensor(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))     # get batch_size starting points\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(xb.shape)\n",
    "\n",
    "\n",
    "# x needs to be embedded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1D:\n",
    "    # normalise each batch -> that way to get diff from mean and unit variance\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1): # params\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)    \n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim= True)    # batch mean\n",
    "        xvar = x.var(1, keepdim=True)          # batch var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalise a row, columwise \n",
    "        self.out = self.gamma * xhat + self. beta\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):       \n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "module = LayerNorm1D(100)   # 100 dimensions\n",
    "x = torch.randn(32, 100)    # 32 batch, 100 dims\n",
    "\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.ones(block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # B, T, head_size   \n",
    "        q = self.key(x) # B, T, head_size \n",
    "        v = self.key(x) # B, T, head_size \n",
    "\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1, keepdim=True) # (B, T, head_size) @ (B, head_size, T) -> keep batch dim so (B, T, T)     \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))     # lower triangular matrix (B, T, T)\n",
    "        wei = F.softmax(1, dim = -1)              # normailise \n",
    "        out = wei @ v            # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(4*n_embd, n_embd),\n",
    "                                nn.Dropout(dropout),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # transformer block, attention -> mlp\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)    # create this many heads, fits into n_ebd\n",
    "        self.ffd = FeedForward\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(self.ln1(x))\n",
    "        x = x + self.ffd(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(n_embd, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, block_size)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)       # final layer norm after mlp?\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.token_embedding_table(idx) # embed x     # B, T, C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))   # T, C\n",
    "        x = tok_emb + pos_emb # B, T, C \n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logit.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is B, T indices of context, unembedded\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get block size of last input\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get predicitons\n",
    "            logits, loss = self(idx_cond)   \n",
    "            # last time step?\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            # sample from dist\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # B, 1\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# evaluate the loss on train / val every so often\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ausch\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ausch\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[165], line 11\u001b[0m, in \u001b[0;36mGPT.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 11\u001b[0m     tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# embed x     # B, T, C\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device \u001b[38;5;241m=\u001b[39m device))   \u001b[38;5;66;03m# T, C\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# B, T, C \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ausch\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ausch\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ausch\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ausch\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evaluate the loss on train / val every so often\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine single and multihead in 1 class, all heads in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# masked fill, lower triangle matrix so past tokens cant look ahead to future tokens\n",
    "# lower triangular matrix tells you how much weight to put on each \n",
    "\n",
    "# the Q, K dot product is the weightings for the @ with the values. Basically the difference between what i am \n",
    "# look for and what i contain determines how much weight to place on the values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__():\n",
    "        super()__init__()\n",
    "        token_embedding_table = nn.Embedding(vocab_size, n_embd)        # for each unique token, get its embedding\n",
    "        position_embedding_table = nn.Embedding(block_size, n_emb)      # get a unique embedding for the position of a token in block size\n",
    "        lm_head = nn.Linear(n_embd, )           \n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_embd = token_embedding_table(idx)\n",
    "        pos_embd = position_embedding_table(idx)\n",
    "\n",
    "        x = tok_embd + pos_+ embd\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        lm_head = nn.Linear(n_embd, )\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "\n",
    "\n",
    "        B, T, C = 4, 8, 32\n",
    "        x = torch.randn(B, T, C)\n",
    "\n",
    "        # each head learns it own unique projection of the embeddings down to its head size.\n",
    "        # different heads hopefully capture different features\n",
    "        key = nn.Linear(C, head_size, bias=False)\n",
    "        query = nn.Linear(C, head_size, bias=False)\n",
    "        value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "        k = key(x)\n",
    "        q = query(x)\n",
    "        v = value(x)\n",
    "\n",
    "        # k and q are B, T, head_size\n",
    "        # only transpose time and head dims\n",
    "        # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "        # T, T is the affinities matrix\n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # then do lower triangle masking\n",
    "        tril = torch.tril(torch.ones(T, T))\n",
    "        wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)           # softmax horizontally, which is across dim1, not down dim 0\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0627e-03, -3.2660e-03, -5.1835e-03,  ...,  2.5971e-03, -1.0526e-02, -7.4462e-03],\n",
      "         [ 1.7579e-02, -2.0755e-02, -3.0835e-02,  ...,  1.6845e-02, -6.8175e-02, -4.9836e-02],\n",
      "         [ 1.0973e-03, -9.4295e-04, -1.3573e-03,  ...,  5.0895e-05, -2.1686e-03, -8.4499e-04],\n",
      "         ...,\n",
      "         [-7.7978e-02,  7.4575e-02,  5.0641e-02,  ...,  2.0182e-01,  2.6490e-01,  3.8846e-01],\n",
      "         [-1.4495e-01, -3.5787e-01,  8.8779e-01,  ...,  2.7113e-01,  8.1441e-01,  1.4782e-01],\n",
      "         [-6.6962e-01, -1.3192e+00, -3.4279e-01,  ...,  8.3123e-01,  1.2434e+00, -8.8896e-01]],\n",
      "\n",
      "        [[-7.1018e-03, -2.9788e-02, -4.1602e-02,  ...,  5.8691e-03, -1.1214e-01,  2.1592e-01],\n",
      "         [ 6.0431e-02, -1.2342e-01, -8.2011e-02,  ...,  8.5697e-03, -2.1339e-01,  5.0892e-01],\n",
      "         [-1.9269e-01,  3.4826e-01, -8.9946e-01,  ..., -5.2624e-01, -4.4395e-01,  7.1422e-02],\n",
      "         ...,\n",
      "         [ 6.9391e-01, -8.0423e-01,  2.2054e+00,  ...,  2.6638e-01, -7.1175e-01,  1.7694e-01],\n",
      "         [ 1.7009e-01,  1.0827e+00,  2.1955e+00,  ...,  8.7532e-01, -8.9743e-01,  2.0894e+00],\n",
      "         [-1.2781e+00,  1.4014e+00,  7.4118e-01,  ...,  1.1153e+00, -1.5436e-01,  1.5789e+00]],\n",
      "\n",
      "        [[ 2.1394e-02,  6.0787e-02, -3.3084e-02,  ...,  2.7791e-02,  3.9667e-02, -5.4947e-02],\n",
      "         [ 4.6855e-02, -3.3099e-02, -4.9640e-02,  ..., -2.1394e-02, -1.3165e-02, -1.9318e-02],\n",
      "         [ 4.9798e-01, -5.3082e-01, -4.1571e-01,  ..., -2.7248e-01, -3.4367e-01, -6.4345e-02],\n",
      "         ...,\n",
      "         [ 1.2984e+00, -7.5812e-01,  1.0688e-01,  ...,  8.8136e-02, -2.0807e-01, -5.8449e-01],\n",
      "         [ 2.2186e+00, -2.1214e+00, -4.1788e-02,  ..., -2.4683e-02,  3.7415e-01, -4.0914e-01],\n",
      "         [ 2.4566e+00, -2.2828e-02, -5.0036e-01,  ...,  3.6238e-01,  1.8648e+00, -9.2778e-02]],\n",
      "\n",
      "        [[ 9.3567e-04, -1.5150e-02, -7.6608e-03,  ...,  1.0753e-02, -4.4018e-03, -1.0738e-02],\n",
      "         [ 3.3316e-02, -1.6695e-01, -6.7896e-02,  ...,  1.4531e-01, -4.7037e-02, -1.4354e-01],\n",
      "         [ 1.1749e-02,  9.3122e-03,  9.6096e-03,  ...,  4.3566e-03,  9.2771e-04, -3.6577e-03],\n",
      "         ...,\n",
      "         [ 8.3626e-01,  1.3643e-01,  3.2530e-01,  ...,  5.7942e-01,  1.4037e-01,  1.1488e-01],\n",
      "         [-1.3352e-01,  4.5272e-01, -4.0853e-01,  ...,  1.0059e-01, -7.0906e-01, -7.2092e-01],\n",
      "         [-1.0201e+00,  2.5826e+00, -3.1148e-01,  ..., -6.8968e-01, -1.0279e+00, -3.3626e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "head_size = 16\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# each head learns it own unique projection of the embeddings down to its head size.\n",
    "# different heads hopefully capture different features\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "# k and q are B, T, head_size\n",
    "\n",
    "# only transpose time and head dims\n",
    "# (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "# T, T is the affinities matrix\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "# then do lower triangle masking\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)           # softmax horizontally, which is across dim1, not down dim 0\n",
    "out = wei @ x\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
